<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7CNoto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;hyacinth.fit&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:true,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:true,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;buttons&quot;,&quot;active&quot;:&quot;gitalk&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null,&quot;activeClass&quot;:&quot;gitalk&quot;},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script><script src="/js/config.js"></script><meta name="description" content="本章我们了解基础的 HTTP 库，如 urllib、requests、httpx等，最后一起来完成对一个静态网站的爬取案例实战。"><meta property="og:type" content="article"><meta property="og:title" content="爬虫笔记二 -- 基本库的使用"><meta property="og:url" content="https://hyacinth.fit/archives/50c99347.html"><meta property="og:site_name" content="Hyacinthの博客"><meta property="og:description" content="本章我们了解基础的 HTTP 库，如 urllib、requests、httpx等，最后一起来完成对一个静态网站的爬取案例实战。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s2.loli.net/2022/02/15/2z4tLguB3OP8nN6.png"><meta property="og:image" content="https://s2.loli.net/2022/02/15/JdV92QIylBomrgU.png"><meta property="og:image" content="https://s2.loli.net/2022/02/15/kbYzSNa8wFvcWsC.png"><meta property="og:image" content="https://s2.loli.net/2022/02/16/1SDINWzmiwVCBEx.png"><meta property="og:image" content="https://s2.loli.net/2022/02/16/MAkavb7fxm689wj.png"><meta property="og:image" content="https://s2.loli.net/2022/03/02/NhWSFV5BXrAwpoy.png"><meta property="article:published_time" content="2022-02-15T01:09:41.000Z"><meta property="article:modified_time" content="2022-03-02T01:42:30.792Z"><meta property="article:author" content="Hyacinth"><meta property="article:tag" content="爬虫笔记"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s2.loli.net/2022/02/15/2z4tLguB3OP8nN6.png"><link rel="canonical" href="https://hyacinth.fit/archives/50c99347.html"><script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;hyacinth.fit&#x2F;archives&#x2F;50c99347.html&quot;,&quot;path&quot;:&quot;archives&#x2F;50c99347.html&quot;,&quot;title&quot;:&quot;爬虫笔记二 -- 基本库的使用&quot;}</script><script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script><title>爬虫笔记二 -- 基本库的使用 | Hyacinthの博客</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="alternate" href="/atom.xml" title="Hyacinthの博客" type="application/atom+xml"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}</style></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Hyacinthの博客</h1><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">记录点滴日常</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#urllib-%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">1.</span> <span class="nav-text">urllib 库的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#requests-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">requests 的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">3.</span> <span class="nav-text">正则表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#httpx-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">httpx 的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%88%98"><span class="nav-number">5.</span> <span class="nav-text">实战</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hyacinth" src="/images/rotate.gif"><p class="site-author-name" itemprop="name">Hyacinth</p><div class="site-description" itemprop="description">平常心 普通人</div></div><div class="site-state-wrap site-overview-item animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/SpeedPromise" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SpeedPromise" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:teemopro@163.com" title="E-Mail → mailto:teemopro@163.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){var o,a,r,f,t,i=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],e=document.getElementById("canvas");function h(t,e){for(var n=[1,2,3],l=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],o=0;o<i[e].length;o++)for(var a,h=0;h<i[e][o].length;h++)1==i[e][o][h]&&(a={x:14*(f+2)*t+2*h*(f+1)+(f+1),y:2*o*(f+1)+(f+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*n[Math.floor(Math.random()*n.length)],color:l[Math.floor(Math.random()*l.length)],disY:1},r.push(a))}function n(){e.height=100;for(var t=0;t<a.length;t++)!function(t,e){for(var n=0;n<i[e].length;n++)for(var l=0;l<i[e][n].length;l++)1==i[e][n][l]&&(o.beginPath(),o.arc(14*(f+2)*t+2*l*(f+1)+(f+1),2*n*(f+1)+(f+1),f,0,2*Math.PI),o.closePath(),o.fill())}(t,a[t]);for(t=0;t<r.length;t++)o.beginPath(),o.arc(r[t].x,r[t].y,f,0,2*Math.PI),o.fillStyle=r[t].color,o.closePath(),o.fill()}e.getContext&&(o=e.getContext("2d"),e.height=100,e.width=700,o.fillStyle="#f00",o.fillRect(10,10,50,50),a=[],r=[],f=e.height/20-1,t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6]),clearInterval(void 0),setInterval(function(){!function(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),n=[];n.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var l=a.length-1;0<=l;l--)n[l]!==a[l]&&t.push(l+"_"+(Number(a[l])+1)%10);for(l=0;l<t.length;l++)h.apply(this,t[l].split("_"));a=n.concat()}(),function(){for(var t=0;t<r.length;t++)r[t].stepY+=r[t].disY,r[t].x+=r[t].stepX,r[t].y+=r[t].stepY,(r[t].x>700+f||r[t].y>100+f)&&(r.splice(t,1),t--)}(),n()},50))}()</script><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-history fa-" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list"><li><a href="/archives/ad504db1.html" title="爬虫笔记十 -- 模拟登录" target="_blank">爬虫笔记十 -- 模拟登录</a></li><li><a href="/archives/a900817c.html" title="爬虫笔记九 -- 代理池的搭建及实战应用" target="_blank">爬虫笔记九 -- 代理池的搭建及实战应用</a></li><li><a href="/archives/410e6558.html" title="爬虫笔记八 -- 验证码的识别" target="_blank">爬虫笔记八 -- 验证码的识别</a></li><li><a href="/archives/d1c41f0f.html" title="爬虫笔记七 -- JavaScript 动态渲染页面爬取" target="_blank">爬虫笔记七 -- JavaScript 动态渲染页面爬取</a></li><li><a href="/archives/57744bfb.html" title="爬虫笔记六 -- 异步爬虫" target="_blank">爬虫笔记六 -- 异步爬虫</a></li></ul></div><div id="music163player"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="280" height="52" src="//music.163.com/outchain/player?type=2&id=1808492017&auto=0&height=32"></iframe></div></div></div></div></aside><div class="sidebar-dimmer"></div></header><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/SpeedPromise" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener external nofollow noreferrer" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hyacinth.fit/archives/50c99347.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/rotate.gif"><meta itemprop="name" content="Hyacinth"><meta itemprop="description" content="平常心 普通人"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Hyacinthの博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">爬虫笔记二 -- 基本库的使用</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-02-15 09:09:41" itemprop="dateCreated datePublished" datetime="2022-02-15T09:09:41+08:00">2022-02-15</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a> </span></span><span id="/archives/50c99347.html" class="post-meta-item leancloud_visitors" data-flag-title="爬虫笔记二 -- 基本库的使用" title="阅读次数"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span></div><div class="post-meta"><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>12k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>11 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><img data-src="https://s2.loli.net/2022/02/15/2z4tLguB3OP8nN6.png" style="zoom:67%"><p>本章我们了解基础的 HTTP 库，如 urllib、requests、httpx等，最后一起来完成对一个静态网站的爬取案例实战。</p><span id="more"></span><h3 id="urllib-库的使用"><a href="#urllib-库的使用" class="headerlink" title="urllib 库的使用"></a>urllib 库的使用</h3><p>urllib 库包含 4 个模块：request、error、parse、robotparser，具体用法如下：</p><ol><li><strong>发送请求</strong></li></ol><p>使用 request 库，可以很方便的发起请求并得到响应。</p><ul><li><strong>urlopen</strong></li></ul><p>以爬取 Python 官网为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img data-src="https://s2.loli.net/2022/02/15/JdV92QIylBomrgU.png" alt="image-20220215101452116"></p><p>可看到我们得到了网页的源代码，其中的链接、图片、文本信息就可以提取出来了。</p><p>使用 <code>print(type(response))</code>，输出结果是<code>&lt;class &#39;http.client.HTTPResponse&#39;</code>，响应是一个 HTTPResponse 类型的对象，主要包含 read、readinto、getheader、getheaders、fileno 等方法，以及 msg、version、status、reason、debuglevel、closed 等属性，我们可以调用上述方法和属性，得到返回的一系列信息。</p><p>urlopen最基本用法就是传入 URL 参数，其完整 API 如下：</p><p><code>urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)</code></p><p>下面详细说明参数含义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data：添加该参数时，需要使用 bytes 方法将其参数转化为字节流编码格式的内容，即 bytes 类型。另外，如果传递了这个参数，，请求方式就变为 POST 了。urllib.parse 模块中 的urlencode(data, encode&#x3D;‘utf-8’)可以转化编码，第二个参数指定编码方式；</span><br><span class="line"></span><br><span class="line">timeout：超时时间，单位秒，超时会抛出 socket.timeout 类型的异常</span><br><span class="line"></span><br><span class="line">其他参数：context 参数必须是 ssl.SSLContext 类型，用来指定 SSL 的设置。cafile 和 capath 用来指定 CA 证书和其路径。cadefault 参数已弃用，默认 False。</span><br></pre></td></tr></table></figure><ul><li><strong>Request</strong></li></ul><p>urlopen 发起的是最基本的请求，如果需要加上 Headers 等信息，就得用到 Request 类来构建请求。构造方法如下：</p><p><code>urllib.request.Request(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">url 和 data 参数解释同 urlopen</span><br><span class="line"></span><br><span class="line">headers：请求头，这也可通过调用请求实例的 add_headers 方法添加。最常见的方法就是通过修改 User-Agent 来伪装浏览器，默认的 UA 是 Python-urllib。如可设置 UA 为 Mozilla&#x2F;5.0 (X11; U; Linux i686) Gecko&#x2F;20071127 FireFox&#x2F;2.0.0.11</span><br><span class="line"></span><br><span class="line">origin_req_host：请求方的 host 名称或 IP 地址</span><br><span class="line"></span><br><span class="line">unverifiable：表示请求是否是无法验证的，默认 False，为 True 则表示用户没有权限来接收这个请求结果</span><br><span class="line"></span><br><span class="line">method：指示请求使用的方法，如 GET、POST 和 PUT 等</span><br><span class="line"></span><br><span class="line">其他参数：context 参数必须是 ssl.SSLContext 类型，用来指定 SSL 的设置。cafile 和 capath 用来指定 CA 证书和其路径。cadefault 参数已弃用，默认 False。</span><br></pre></td></tr></table></figure><p>下面尝试构建请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/post&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Chrome/98.0.4758.80 Safari/537.36 Edg/98.0.1108.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.httpbin.org&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">query = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(query), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url, data, headers, method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;germey&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;identity&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;11&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Chrome/98.0.4758.80 Safari/537.36 Edg/98.0.1108.50&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-620b1624-3d236f007cd3de6c15dfe6ba&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;117.136.71.145&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>高级用法</strong></li></ul><p>为了进行 Cookie 处理、代理设置等操作，就绪也用到 urllib.request 中的 BaseHandler 类，有各种子类继承 BaseHandler：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HTTPDefaultErrorHandler 用于处理响应错误，所有错误会抛出 HTTPError 类型异常</span><br><span class="line">HTTPRedirectHandler 用于处理重定向</span><br><span class="line">HTTPCookieProcessor 用于处理 Cookie</span><br><span class="line">ProxyHandler 用于设置代理，默认为空</span><br><span class="line">HTTPPasswordMgr 用于管理密码</span><br><span class="line">HTTPBasicAuthHandler 用于管理认证</span><br></pre></td></tr></table></figure><ul><li><strong>代理</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://127.0.0.1:8080&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://127.0.0.1:8080&#x27;</span>,</span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.<span class="built_in">open</span>(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason</span><br></pre></td></tr></table></figure><ul><li><strong>Cookie</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;cookie.txt&#x27;</span></span><br><span class="line">cookie = http.cookiejar.CookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>) <span class="comment"># save as file</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name + <span class="string">&#x27;=&#x27;</span> + item.value)</span><br><span class="line"> </span><br><span class="line">------------- result -------------</span><br><span class="line">BAIDUID=BB78F3CB65EB85782F1ECF6B3C04D0DB:FG=<span class="number">1</span></span><br><span class="line">BIDUPSID=BB78F3CB65EB8578C40219B54CB0A2E7</span><br><span class="line">PSTM=<span class="number">1644894666</span></span><br><span class="line">BD_NOT_HTTPS=<span class="number">1</span></span><br></pre></td></tr></table></figure><ol start="2"><li><strong>处理异常</strong></li></ol><ul><li><strong>URLError</strong></li></ul><p>来自 urllib库的 error 模块，继承自 OSError 模块，由 request 模块产生的异常都可以用这个类处理。</p><ul><li><strong>HTTPError</strong><ul><li>code：HTTP状态码</li><li>reason：返回错误原因</li><li>headers：返回请求头</li></ul></li></ul><p>URLError 的子类，专门用于处理 HTTP 请求错误，有 3 个属性。</p><ol start="3"><li><strong>解析链接</strong></li></ol><p>本小节介绍 parse 模块的常用方法。</p><ul><li><strong>urlparse</strong></li></ul><p>API <code>urllib.parse.urlparse(urlstring, scheme=&#39;&#39;, allow_fragments=True)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">urlstring：必填项，待解析 URL</span><br><span class="line">scheme：默认协议（如 http、https 等）。如果 urlstring 没有带协议值，则赋该值</span><br><span class="line">allow_fragments：是否忽略 fragment，如果忽略，则会被解析为 path、params 或 query 的一部分</span><br><span class="line">----------------------------------</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">&#x27;https://www.baidu.com/index.html;user?a=6#comment&#x27;</span>)</span><br><span class="line">print(<span class="built_in">type</span>(result))</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">urllib</span>.<span class="title">parse</span>.<span class="title">ParseResult</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">ParseResult</span>(<span class="params">scheme=<span class="string">&#x27;https&#x27;</span>, netloc=<span class="string">&#x27;www.baidu.com&#x27;</span>, path=<span class="string">&#x27;/index.html&#x27;</span>, params<span class="string">&#x27;user&#x27;</span>, query=<span class="string">&#x27;id=5&#x27;</span>, fragment=<span class="string">&#x27;comment&#x27;</span></span>)</span></span><br></pre></td></tr></table></figure><ul><li><strong>urlunparse</strong></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line">data = [<span class="string">&#x27;https&#x27;</span>, <span class="string">&#x27;www.baidu.com&#x27;</span>, <span class="string">&#x27;index.html&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;a=6&#x27;</span>, <span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">https://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure><ul><li><strong>urlsplit、urlunsplit</strong></li></ul><p>和 urlparse、urlunparse 很相似，但不在单独解析 params，也不用组合 params内容 。</p><ul><li><strong>urljoin</strong></li></ul><p>API <code>urllib.parse.urljoin(base_url, new_url)</code></p><p>urljoin 会分析 base_url 中的 scheme、netloc 和 path，并对 new_url 缺失的部分进行补充，最后返回结果。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">urljoin(<span class="string">&#x27;https://www.baidu.com?wd=abc&#x27;</span>, <span class="string">&#x27;https://example.com/index.php&#x27;</span>)</span><br><span class="line">urljoin(<span class="string">&#x27;https://www.baidu.com&#x27;</span>, <span class="string">&#x27;?category=2#comment&#x27;</span>)</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">https://example.com/index.php</span><br><span class="line">https://www.baidu.com?category=2#comment</span><br></pre></td></tr></table></figure><ul><li><strong>urlencode</strong></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: <span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;https;//www.baidu.com&#x27;</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">https;//www.baidu.com?name=germey&amp;age=25</span><br></pre></td></tr></table></figure><ul><li><strong>parse_qs</strong></li></ul><p>反序列化，可将一串 GET 请求参数转化为字典。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">&#x27;name=germey&amp;age=25&#x27;</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;germey&#x27;</span>], <span class="string">&#x27;age&#x27;</span>: [<span class="string">&#x27;25&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>parse_qsl</strong></li></ul><p>与 parse_qs 相似，但运行结果是元组列表，运行结果如下：</p><p><code>&#123;(&#39;name&#39;: &#39;germey&#39;), (&#39;age&#39;: &#39;25&#39;)&#125;</code></p><ul><li><strong>quote</strong></li></ul><p>将内容转化为 URL 编码格式。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">&#x27;壁纸&#x27;</span></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?wd=&#x27;</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure><ul><li><strong>unquote</strong></li></ul><p>URL 解码，用上面代码运行结果可以解码出中文。</p><ul><li><strong>Robots 协议</strong></li></ul><p>也叫做爬虫协议、机器人协议，全名为网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取、哪些不可以。通常是一个 robots.txt 的文本文件，放在网站根目录下。</p><p>样例，限定搜索爬虫只能爬取 public 目录：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">User-Agent: * <span class="comment"># 搜索爬虫名称，常见名称见下表</span></span><br><span class="line">Disallow: /</span><br><span class="line">Allow: /public/</span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">爬虫名称</th><th align="center">网站名称</th></tr></thead><tbody><tr><td align="center">BaiduSpider</td><td align="center">百度</td></tr><tr><td align="center">Googlebot</td><td align="center">谷歌</td></tr><tr><td align="center">360Spider</td><td align="center">360 搜索</td></tr><tr><td align="center">YodaoBot</td><td align="center">有道</td></tr><tr><td align="center">ia_archiver</td><td align="center">Alexa</td></tr><tr><td align="center">Scooter</td><td align="center">altavista</td></tr><tr><td align="center">Bingbot</td><td align="center">必应</td></tr></tbody></table><p>了解 Robots 协议后，就可以使用 robotparser 模块来解析 robots.txt 文件了，用法：</p><p><code>urllib.robotparse.RobotFileParse(url=&#39;&#39;)</code></p><h3 id="requests-的使用"><a href="#requests-的使用" class="headerlink" title="requests 的使用"></a>requests 的使用</h3><p>requests 库比 urllib 库更强大，在处理网页验证、Cookie 以及实现 POST、PUT 等请求会更方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">headers = &#123; <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;...&#x27;</span> &#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: <span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(url, headers=headers, params=data) <span class="comment"># 发起请求，返回结果是 str 类型</span></span><br><span class="line">print(r.json()) <span class="comment"># 如果结果是 JSON 格式，可将结果转化为字典</span></span><br><span class="line">print(r.text()) <span class="comment"># 返回 Unicode 类型数据，即取文本内容</span></span><br><span class="line">print(r.content()) <span class="comment"># 结果转化为 bytes 类型数据 </span></span><br><span class="line">print(r.cookies) <span class="comment"># 获取 Cookie，通过 for key, value in r.cookies.item() 解析</span></span><br><span class="line"><span class="comment"># 登录网站后，将请求头中的 Cookie 内容复制加到爬虫的请求头中，便可获取登录后的信息结果</span></span><br><span class="line">print(r.__dict__) <span class="comment"># 可查看返回结果的所有属性内容</span></span><br><span class="line">print(<span class="built_in">dir</span>(r)) <span class="comment"># 查看所有属性名</span></span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">&#x27;file&#x27;</span>: <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">&#x27;test.com&#x27;</span>, files=files) <span class="comment"># post 请求</span></span><br></pre></td></tr></table></figure><ul><li><strong>Session 维持</strong></li></ul><p>设想，某次利用 post 请求登录网站后，想再次获取登录后的个人信息，使用 get 方法请求，这样是无法获取到的，这相当于开了两个浏览器。</p><p>下面举例 Session 的用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.session()</span><br><span class="line">s.get(<span class="string">&#x27;https://www.httpbin.org/cookies/set/number/123456&#x27;</span>)</span><br><span class="line">r = s.get(<span class="string">&#x27;https://www.httpbin.org/cookies&#x27;</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line">&#123; <span class="string">&quot;cookies&quot;</span>: &#123;<span class="string">&quot;number&quot;</span>: <span class="string">&quot;123456&quot;</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>可看到 Cookie 获取成功，利用 Session 可以模拟在同一个浏览器中打开同一站点的不同页面。</p><ul><li><strong>其他参数配置</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">verify： 设置为 False 可忽略 SSL 证书的验证</span><br><span class="line">timeout：超时设置，可设置为固定值，也可是元组（请求实际可分为连接和读取两个阶段，超时时间可自定）</span><br><span class="line">auth：身份认证，传递元组 (&#39;user&#39;, &#39;password&#39;)</span><br><span class="line">proxies：代理设置，如果代理需要使用身份认证，可使用类似 http:&#x2F;&#x2F;user:password@host:port 语法设置</span><br></pre></td></tr></table></figure><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>用一定语法规则将特定文本提取出来的方式就是正则表达式匹配，常用匹配规则见<a target="_blank" rel="noopener external nofollow noreferrer" href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Guide/Regular_Expressions">正则表达式规则</a>。</p><ul><li><strong>match</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span></span><br><span class="line">print(<span class="built_in">len</span>(content))</span><br><span class="line">result = re.match(<span class="string">&#x27;Hello\s\d\d\d\s(\d&#123;4&#125;)\s\w&#123;10&#125;&#x27;</span>, content)</span><br><span class="line">print(result) <span class="comment"># \s 匹配空白字符，\d 匹配数字，\w 匹配单字字符（字母、数字或者下划线）</span></span><br><span class="line">print(result.group())</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line">print(result.span())</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line"><span class="number">41</span></span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">25</span>), match=<span class="string">&#x27;Hello 123 4567 World_This&#x27;</span>&gt;</span><br><span class="line">Hello <span class="number">123</span> <span class="number">4567</span> World_This</span><br><span class="line"><span class="number">4567</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>从字符串开头开始匹配，第一个参数传入正则表达式，第二个参数传入要匹配的字符串。用 () 将想提取的字符串括起来，匹配结果调用 group 方法，传入分组索引即可获取提取结果。另外<code>.*</code>是通用匹配，代码中的表达式也可以变为 <code>^Hello.*Demo$</code>，<code>^ $</code> 表示开始和结束标识。</p><p><strong>贪婪与非贪婪</strong></p><p>来看一种情况，想要获取目标字符串中的数字，用 <code>.*</code> 简化成下面的表达式，运行如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span></span><br><span class="line">result = re.match(<span class="string">&#x27;^Hello.*(\d+).*Demo$&#x27;</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">------------- result -------------</span><br><span class="line"><span class="number">41</span></span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">41</span>), match=<span class="string">&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span>&gt;</span><br><span class="line"><span class="number">7</span></span><br></pre></td></tr></table></figure><p>只得到了数字 7，这是因为前面的 <code>.*</code> 会匹配尽可能多的字符，只给 \d+ 剩下一个可满足条件的数字 7，表现出贪婪性，这里我们需要将其变成非贪婪匹配，写法是 <code>.*?</code>，如此运行结果就正常了。</p><p><strong>修饰符</strong></p><p>在网页匹配过程中会遇到换行符等情况，这时候运行就会报错，因为无法匹配换行符，解决方法是 match 加上第三个参数 re.S，常见的修饰符见下表：</p><table><thead><tr><th align="center">修饰符</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">re.I</td><td align="center">使匹配对大小写不敏感</td></tr><tr><td align="center">re.L</td><td align="center">实现本地化识别（locale-aware）匹配</td></tr><tr><td align="center">re.M</td><td align="center">多行匹配，影响 ^ 和 $</td></tr><tr><td align="center">re.S</td><td align="center">使匹配内容包括换行符在内的所有字符</td></tr><tr><td align="center">re.U</td><td align="center">根据 Unicode 字符集解析字符，影响 \w、\W、\b 和 \B</td></tr><tr><td align="center">re.X</td><td align="center">忽略表达式中空白和注释（# 引导），以便将正则表达式写得更易于理解</td></tr></tbody></table><p><strong>转移匹配</strong></p><p>当在目标字符串中遇到用作正则匹配模式的特殊字符时，在此字符前加 \ 转义即可。</p><ul><li><strong>search</strong></li></ul><p>匹配时会扫描整个字符串，返回第一个匹配成功的结果，为了匹配方便，尽量使用、 search()。</p><ul><li><strong>findall</strong></li></ul><p>返回所有匹配成功的结果，结果是列表类型，列表中每个元素则是元组类型。</p><ul><li><strong>sub</strong></li></ul><p>正则替换，<code>re.sub(&#39;\d+&#39;, &#39;str&#39;, content)</code>，解释：在 content 中匹配所有数字，将其替换为 str 字符（可为空）。很多时候爬取下来的数据都需要进行清洗，再进行匹配提取操作，sub 方法是不错的选择。</p><ul><li><strong>compile</strong></li></ul><p>为了复用正则表达式，可以用 compile 方法给表达式做一层封装，方便使用，且 compile 中还可加入修饰符。</p><h3 id="httpx-的使用"><a href="#httpx-的使用" class="headerlink" title="httpx 的使用"></a>httpx 的使用</h3><p>urllib 和 requests 有个问题就是只支持 HTTP/1.1，对采用 HTTP/2.0 的网站无法爬取数据，目前支持 HTTP/2.0 有代表性的库有 hyper 和 httpx，后者使用更方便，功能也更强大，用法上和 requests 基本差不多，这里主要说明不一样的地方。</p><ul><li><strong>Client 对象</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> httpx.Client() <span class="keyword">as</span> client:</span><br><span class="line">    response = client.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>)</span><br><span class="line"><span class="comment"># 推荐 with as 用法，等价于：</span></span><br><span class="line">client = httpx.Client()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = client.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    client.close()</span><br></pre></td></tr></table></figure><p>声明 Client 对象时也可指定 headers 等参数，如果开启 HTTP/2.0（默认关闭），写法为：</p><p><code>client = httpx.Client(http2=True)</code></p><blockquote><p>注意：启用 HTTP/2.0 前必须保证客户端和服务器都支持 HTTP/2.0，不然就得改用 1.1 版本</p></blockquote><ul><li>支持异步请求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> httpx.AsyncClient(http2=<span class="literal">True</span>) <span class="keyword">as</span> client:</span><br><span class="line">        response = client.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(fetch(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>))</span><br></pre></td></tr></table></figure><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>本节爬取一个基本的静态网站来练手。</p><p><strong>链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://ssr1.scrape.center/">https://ssr1.scrape.center/</a></p><p><img data-src="https://s2.loli.net/2022/02/15/kbYzSNa8wFvcWsC.png"></p><p><strong>目标</strong></p><ul><li>使用 requests 爬取电影列表，包括每个电影的详情页；</li><li>用正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等；</li><li>爬取的内容保存为 JSON 文本文件；</li><li>使用多线程加速爬取</li></ul><p><strong>分析</strong></p><p>进入网页后，打开开发者工具（F12），如下图：</p><p><img data-src="https://s2.loli.net/2022/02/16/1SDINWzmiwVCBEx.png"></p><p>可以看到电影名称就是一个 h2 节点文本，父节点是一个 a 节点，带有 href 属性，表示的是电影详情页的 URL，其他的电影信息都可以通过提取源代码节点中的内容来获得。下面，再来看分页逻辑，将页面拉到底，点击第 2 页，可看到跳转链接为原链接加上 /page/2，继续点击第 3 页、第 4 页，发现逻辑一样，分页思路就清晰了。</p><p><img data-src="https://s2.loli.net/2022/02/16/MAkavb7fxm689wj.png"></p><p>要完成列表页的爬取，我们可以这么实现：</p><ul><li>遍历所有页面，索引为1-10</li><li>从每个页面提取每个电影的详情页 URL</li><li>提取每个详情页的页面元素信息并存储</li></ul><p>具体实现中，我们使用 requests 获取源代码，用 re 库采用正则表达式提取有关信息，并保存为 json 格式的 txt 文本，代码中用到了 logging 库输出信息，可区分内容级别，相比 print 输出定制化程度更高，且线程安全。还用到了 multiprocessing 库实现多进程爬取，全部代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> captcha.image <span class="keyword">import</span> ImageCaptcha</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> setting</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line">count_single = <span class="number">10</span>  <span class="comment"># 单条进程生成数量</span></span><br><span class="line">total_process = <span class="number">10</span>  <span class="comment"># 进程数目</span></span><br><span class="line">path = setting.TRAIN_DATASET_PATH  <span class="comment"># 图片数据存储路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_captcha_text</span>():</span></span><br><span class="line">    captcha_text = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(setting.MAX_CAPTCHA):</span><br><span class="line">        c = random.choice(setting.ALL_CHAR_SET)</span><br><span class="line">        captcha_text.append(c)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(captcha_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_captcha_text_and_image</span>():</span></span><br><span class="line">    image = ImageCaptcha()</span><br><span class="line">    captcha_text = generate_captcha_text()</span><br><span class="line">    captcha_image = Image.<span class="built_in">open</span>(image.generate(captcha_text))</span><br><span class="line">    <span class="keyword">return</span> captcha_text, captcha_image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dataset</span>(<span class="params">_</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count_single):</span><br><span class="line">        now = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time()))</span><br><span class="line">        text, image = generate_captcha_text_and_image()</span><br><span class="line">        filename = text + <span class="string">&#x27;_&#x27;</span> + now + <span class="string">&#x27;.png&#x27;</span></span><br><span class="line">        image.save(path + os.path.sep + filename)</span><br><span class="line">        print(<span class="string">&#x27;saved %d : %s &#x27;</span> % (i + <span class="number">1</span>, filename))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pool = multiprocessing.Pool()</span><br><span class="line">    processes = <span class="built_in">range</span>(total_process + <span class="number">1</span>)</span><br><span class="line">    pool.<span class="built_in">map</span>(generate_dataset, processes)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><p>考虑到数据较多，这里还使用了多进程，生成的数据集如下：</p><p><img data-src="https://s2.loli.net/2022/03/02/NhWSFV5BXrAwpoy.png"></p></div><div class="popular-posts-header">推荐阅读</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="\archives\fc84b201.html" rel="bookmark">爬虫笔记一 -- 爬虫基础</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="\archives\d1c41f0f.html" rel="bookmark">爬虫笔记七 -- JavaScript 动态渲染页面爬取</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="\archives\79659fc4.html" rel="bookmark">爬虫笔记三 -- 解析库的使用</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="\archives\e6a92538.html" rel="bookmark">Python 批量重命名文件</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="\archives\a900817c.html" rel="bookmark">爬虫笔记九 -- 代理池的搭建及实战应用</a></div></li></ul><footer class="post-footer"><div class="reward-container"><div>Buy me a coffee</div><button onclick='document.querySelector(".post-reward").classList.toggle("active")'>赞赏</button><div class="post-reward"><div><img src="/images/wechat-pay.png" alt="Hyacinth 微信"> <span>微信</span></div><div><img src="/images/alipay-pay.png" alt="Hyacinth 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>Hyacinth</li><li><strong>修改时间：</strong> <time title="修改时间：2022-03-02 09:42:30" itemprop="dateModified" datetime="2022-03-02T09:42:30+08:00">2022/03/02</time></li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hyacinth.fit/archives/50c99347.html" title="爬虫笔记二 -- 基本库的使用">https://hyacinth.fit/archives/50c99347.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><span>欢迎关注我的其它发布渠道</span><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/images/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i> </span><span class="label">WeChat</span></a></div><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i> </span><span class="label">RSS</span></a></div></div></div><div class="post-tags"><a href="/tags/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 爬虫笔记</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/fc84b201.html" rel="prev" title="爬虫笔记一 -- 爬虫基础"><i class="fa fa-chevron-left"></i> 爬虫笔记一 -- 爬虫基础</a></div><div class="post-nav-item"><a href="/archives/79659fc4.html" rel="next" title="爬虫笔记三 -- 解析库的使用">爬虫笔记三 -- 解析库的使用 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments gitalk-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Hyacinth</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span>站点总字数：</span> <span title="站点总字数">276k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span>站点阅读时长 &asymp;</span> <span title="站点阅读时长">4:11</span></span></div></div></footer><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>// 代码折叠<script src="/js/code-unfold.js"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;mXCXRvyDJTe1MizGNz8MBQCC-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;NKjSKtz16WlOVxKDRa1tJNgl&quot;,&quot;server_url&quot;:null,&quot;security&quot;:false}</script><script src="/js/third-party/statistics/lean-analytics.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script><script src="/js/third-party/math/mathjax.js"></script><script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script><script>var options={bottom:"64px",right:"unset",left:"32px",time:"0.5s",mixColor:"transparent",backgroundColor:"transparent",buttonColorDark:"#100f2c",buttonColorLight:"#fff",saveInCookies:!0,label:"🌓",autoMatchOsTheme:!0};const darkmode=new Darkmode(options);window.darkmode=darkmode,darkmode.showWidget()</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous"><script class="next-config" data-name="gitalk" type="application/json">{&quot;enable&quot;:true,&quot;github_id&quot;:&quot;SpeedPromise&quot;,&quot;repo&quot;:&quot;blog-comments&quot;,&quot;client_id&quot;:&quot;c87c14d15109ca258c8c&quot;,&quot;client_secret&quot;:&quot;fcb23a076c153b6a22f7dc957f54510992dc0342&quot;,&quot;admin_user&quot;:&quot;SpeedPromise&quot;,&quot;distraction_free_mode&quot;:true,&quot;proxy&quot;:&quot;https:&#x2F;&#x2F;cors-anywhere.azm.workers.dev&#x2F;https:&#x2F;&#x2F;github.com&#x2F;login&#x2F;oauth&#x2F;access_token&quot;,&quot;language&quot;:null,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;gitalk@1.7.2&#x2F;dist&#x2F;gitalk.min.js&quot;,&quot;integrity&quot;:&quot;sha256-Pmj85ojLaPOWwRtlMJwmezB&#x2F;Qg8BzvJp5eTzvXaYAfA&#x3D;&quot;},&quot;path_md5&quot;:&quot;82bac9134e1962af6fcfd5ae60f47246&quot;}</script><script src="/js/third-party/comments/gitalk.js"></script><script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><script async src="/js/cursor/fireworks.js"></script></body></html>